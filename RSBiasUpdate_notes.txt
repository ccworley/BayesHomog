19/09/2018 RS Code Updates

The main point was to abandon the idea of fitting the bias as a function of the inferred best values of Teff of the benchmarks. Trying to do it like that made the problem become extremely correlated. The bias depended on the inferred best value, which in turn depended on the bias correction. So now I am writing the bias as a function of the given benchmark values of Teff. That is not the ideal way, but should be a good approximation if the given values are close to the best values, which should be true most of the time.

The second thing is about normalizing the Teff values. Well, I am still not sure if there is a good way to normalize the node observations. So I just did not do it. But, I am normalizing the given benchmark values for the sake of fitting the bias. The secret is just to "un-normalize" the bias when comes the moment to use it inside the normal distribution.

Let me know if this is not clear. I include the changes in the code below.

I also attach some plots. For three nodes showing the bias function that was fitted (in red) together with the 95% confidence interval (in blue).

Also, 3 plots with the difference between given Teff and best Teff as a function of the atmospheric parameters. There are no correlations left, but there is still a scatter of about \pm 75 K (standard deviation). And there are two metal-poor benchmarks that do not conform to the rest.

I hope this solution will also work for you.

There is still something I'd like to test with the bias and then I will attack the problem of the covariance matrix again.

--------------------------------------------
--------------------------------------

*) Outside the model, we compute the mean and sd of the Teff values:

mean.bench.teff <- mean(given.teff.bench)
sd.bench.teff <- sd(given.teff.bench)

*) Pass those as part of the data:

teff.data <- list(
                  ...
                  #For normalizing Teff values and estimating biases
                  offset.teff = mean.bench.teff,
                  scale.teff = sd.bench.teff
                           )

*) Use the uniform prior for the missing data

#Prior on the missing values

for (l in 1:n.missing) { # When the node was missing a value (n.missing), we include a value drawn from the broad prior

true.teff.benchmark.per.spec[i.of.missing.values[l],j.of.missing.values[l]] ~ dunif(3000,8000)
}

- The normal prior is faster, but it causes problems. Basically, the model starts to think that all benchmarks should have Teff around the mean (~5800 K) and then computes very strong biases when the Teff is very different from that.

*) Prior for the coefficients of the bias fitting. Now that we are using normalized Teff values to compute the bias, there is no problem with choosing the prior for these alpha coefficients


# Other Priors needed for coefficients
# Priors on the coefficients alpha for the fit of the bias function. We fit one bias function per node. The bias.teff is for now a quadratic function of the true.teff
    for (l in 1:K) {       # On number of nodes
       for (m in 1:P) {    # On number of setups
          for (j in 1:3) { # 3 coefficients needed for fitting a quadratic function
              alpha[j,l,m] ~ dnorm(0,pow(0.1,-2)) # Remember here that dnorm(mu,tau) where mu = mean and tau = 1/sigma^2, and sigma is the standard deviation
           }
        }
    }


*) Normalize the benchmark Teff values for use when fitting the bias function. And use the given.teff of the benchmarks

# I will eventually need also a vector that really has only the the.true.teff.bench[i], without taking into account the missing values, for the fitting of the bias function (see below)

for (i in 1:M) {
   for (j in 1:K) {
        only.the.true.teff.benchmark[i,j] <- (given.teff.benchmarks[star.code[i]]-offset.teff)/scale.teff
   }
}


*) Fit a normalized bias function, but inside the normal distribution re-scale it back to normal Teff values

#Likelihood - per spectrum (vectorized over K = num.nodes)

for (j in 1:M) {
    observed.node.teff.per.spec[j,1:K] ~ dmnorm( (true.teff.benchmark.per.spec[j,1:K] + (norm.bias.vector.teff.node[j,1:K]*scale.teff)) , teff.InvCovMat[1:K,1:K] )

    norm.bias.vector.teff.node[j,1:K] <- alpha[1,1:K,setup.code[j]] + alpha[2,1:K,setup.code[j]]*only.the.true.teff.benchmark[j,1:K] + alpha[3,1:K,setup.code[j]]*pow(only.the.true.teff.benchmark[j,1:K],2)

}

*) Now, because I changed the way of declaring the alphas, the init function should look like:

# Intial values for free parameters
inits <- function() {
list(alpha = array(rnorm(3*num.setups*num.nodes,0,0.1),dim=c(3,num.nodes,num.setups)),
     teff.InvCovMat = diag(rgamma(num.nodes,0.1,0.1))
)
}

*) The monitor parameter inside the JAGS call becomes:

monitor=c("the.true.teff.bench","node.sd.teff","teff.Rho","alpha")

*) And then the summary would work with:

print(summary(simple_test,vars=c('alpha')))

--------------------------------------


